---
title: "NBA"
author: "Hugo Cesar Octavio del Sueldo"
date: "10/23/2020"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    math: katex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r data, include = FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(knitr)
library(gvlma)
library(MASS)
library(car)
library(bootstrap)
library(prettydoc)
nba <- read_csv("nba.csv")
```
## Prediccion en el Data Set NBA
El data set `nba` muestra informacion sobre los salarios y los rendimientos de los jugadores de la NBA

### Exploratory data analysis of the data set

- Player: Nombre del jugador.

- Salary: Salario en dolares.

- NBA_Country: NAcionalidad del jugador.

- NBA_DraftNumber: Posición del draft.

- Age: Edad del jugador.

- Tm: Abreviatura del nombre.

- G: Partidos jugados

- MP: Minutos jugados.

- PER: Índice de eficiencia del jugador. El promedio de la liga es 15.

- TS = Porcentaje de tiro real. Tiene en cuenta los tiros de 2 puntos, los triples y los tiros libres.
                                        
- TMP: Minutos jugados por el equipo.
                                        
- 3PAr: Porcentaje de triples.
                                        
- FTr: Porcentaje de tiros libres.
                                        
- ORB%Offensive Rebound Percentage (ORB%): Porcentaje de rebotes ofensivos.

- DRB%: Porcentaje de rebote defensivo.

- TRB%: Porcentaje de rebote total.

- AST%: Porcentaje de asistencia.

- STL%: Porcentaje de robo.

- BLK%: Porcentaje de tapones.

- TOV%: Porcentaje de robo de balón antes de que el equipo contrario tire.

- USG%: Porcentaje de jugadas que estuvo involucrado un jugador, siempre que la jugada termine en uno de los tres resultados reales: intento de gol de campo, intento de tiro libre o pérdida.

- OWS: Acciones de victoria ofensivas.

- DWS: Acciones de victorias defensivas.

- WS: Un número estimado de victorias a las que un jugador ha contribuido.

- WS/48: WS por 48 minutos.

- BPM: Calcula el más/menos de un jugador respecto al rendimiento del equipo por cada 100 posesiones ofensivas.

- DBPM: Calcula el más/menos de un jugador respecto al rendimiento del equipo por cada 100 posesiones defensivas.

- BPM: Calcula el más/menos de un jugador respecto al rendimiento del equipo por cada 100 posesiones.

- VORP: Valor sobre jugador de reemplazo.
```{r }
head(nba) #primeras 10 observaciones
tail(nba) #ultimas 10 observaciones
names(nba)
class(nba) #clase de nba
typeof(nba) #tipo de dato interno usado por el objeto
str(nba)
length(nba) #cantidad de columnas
dim(nba) #cantidad de filas y columnas
summary(nba) #summary de los principales estadisticos
```
### Limpieza de valores duplicados o nulos

```{r}
distinct(nba)
distinct(nba, Player)
duplicated(nba)
nrow(nba[duplicated(nba$Player), ]) #cuentame los repetidos
nba <- nba[!duplicated(nba$Player), ] #borro los duplicados y lo guardo con el mismo nombre de mi data set
distinct(nba) #compruebo los resultados

summarise_all(nba, funs(sum(is.na(.)))) #cuentame los valores nulos en el dataset

```
#### Cambio los nombres de algunas columnas

Algunas columnas tienen nombres con los simbolos del %, comienzan con numeros o / que nos afectan a la hora de hacer calculos. Por lo tanto, vamos a modificarlas para poder trabajar correctamente.

```{r}
nba <- rename_with(nba, ~ tolower(gsub('%', '', .x, fixed = T)))
nba <- rename_with(nba, ~ tolower(gsub('3', 'three', .x, fixed = T)))
nba <- rename_with(nba, ~ tolower(gsub('/', '_', .x, fixed = T)))
```

#### Borro los nulos de las columnas que tienen valores NA

```{r}
nba <- nba[!is.na(nba$ts),]
nba <- nba[!is.na(nba$threepar),]
nba <- nba[!is.na(nba$ftr),]
nba <- nba[!is.na(nba$tov),]
```

#### Creamos las variables country y team como factor para poder sumarlas a la regresion

```{r}
nba <- mutate(nba, country = as.numeric(factor(nba$nba_country))) #paises
nba <- mutate(nba, team = as.numeric(factor(nba$tm))) #teams
```


### Regresion lineal de las variables para explicar el salario de los jugadores
```{r}
#Calculo el salario con logaritmo neperiano en la regresion porque existe un rango de valores muy amplio.
regres01 = lm(log(salary, base = 10) ~ nba_draftnumber + age + g + mp + per + ts + threepar + ftr + orb + drb + trb  + ast + stl + blk + tov + usg + ows + dws + ws + ws_48 + obpm + dbpm + bpm + vorp + country + team,data = nba)
gvmodel <- gvlma(regres01) 
summary(gvmodel)

#Aqui vemos que las variables explicativas mas significativa son draftnumber, age, g y mp

```
#### Vamos a probar si merece la pena sumar las variables country y team al modelo 

Para esto utilizaremos la funcion anova para comparar dos modelos donde el primero contiene al segundo.

```{r}
mod <- lm(log(salary, base = 10) ~ nba_draftnumber + age + g + mp + per + ts + threepar + ftr + orb + drb + trb  + ast + stl + blk + tov + usg + ows + dws + ws + ws_48 + obpm + dbpm + bpm + vorp + country + team,data = nba)
mod1 <- lm(log(salary, base = 10) ~ nba_draftnumber + age + g + mp + per + ts + threepar + ftr + orb + drb + trb  + ast + stl + blk + tov + usg + ows + dws + ws + ws_48 + obpm + dbpm + bpm + vorp,data = nba)

anova(mod1, mod)

# vemos que las variables country y team son significativas para estimar el salario de los jugadores ya que al ser el valor de p alto podemos decir que no se pueden descartar el modelo que contenga estas variables
```

### Ahora analizaremos los supuestos que no se cumplen para el modelo de regresion multilineal simple

#### Vamos a analizar la normalidad con QQ-plots

Un QQ-plots (quantile vs quantile plots) es una representación gráfica, que sirve para comparar dos distribuciones y ver si coinciden. En realidad, para comprobar si un conjunto de datos muestrales están generados por una distribución teórica como la normal 
```{r}

qqPlot(regres01, labels = row.names(nba), id.method = "identify",
       simulate = TRUE, main = "Q-Q Plot")
```
#### Vemos el histograma con su funcion de densidad 

```{r}
residplot <- function(fit, nbreaks=20) {
  z <- rstudent(fit)
  hist(z, breaks=nbreaks, freq=FALSE,
       xlab="Studentized Residual",
       main="Distribution of Errors")
  rug(jitter(z), col="brown")
  curve(dnorm(x, mean=mean(z), sd=sd(z)),
        add=TRUE, col="blue", lwd=2)
  lines(density(z)$x, density(z)$y,
        col="red", lwd=2, lty=2)
  legend("topright",
         legend = c( "Normal Curve", "Kernel Density Curve"),
         lty=1:2, col=c("blue","red"), cex=.7)
}

residplot(regres01)
```


#### Linealidad

Se grafican los valores ajustados con respecto a los predictores, si no hay problemas de linealidad se obtiene un recta sobre las que se representan los puntos.

```{r linealidad, warning = FALSE}
crPlots(regres01)
```



#### Multicolinealidad

Es la existencia de alta correlación entre los predictores puede producir problemas de imprecisión de los estimadores (las varianzas de los estimadores son mayores de lo que deberían ser). Así, los intervalos de confianza son muy anchos, hay dificultad para interpretar los coeficientes y se tiende a no rechazar las hipótesis nula de significación.

##### Método del Factor de Inflación de la Varianza

```{r}
vif(regres01)
sqrt(vif(regres01)) > 2

# tenemos problemas de multicolinealidad en ts, trb, usg, obpm, threepar, ows, dbpm, g, dws, bpm, mp, orb, blk, ws, vorp, per, drb, ws_48
```


#### Outliers

##### Observaciones anómalas
1 - Atípicos: Una observación es atípica si el residuo asociado es grande.
2 - Extrema o Apalancada: Una observación es extrema (o potencialmente influyente o apalancada) si se encuentra apreciablemente alejada del resto de observaciones de la muestra.
3 - Influyente: Una observación es influyente si la presencia de dicha observación en la muestra altera significativamente algún aspecto de la estimación del modelo.

###### Valores Atípicos
- Identificamos los valores atípicos mediante un Bonferroni p-values. En este caso solo contrasta el mayor de los residuos, si se rechaza que sea atípico se concluye que no hay atípicos.
- También se puede realizar un gráfico de los residuos estandarizados con ±2σ.

```{r}
outlierTest(regres01)
```
###### Valores extremos
- Para determinar valores extremos, se calcula el `hat statistic`, siendo la media *p/n* donde p es el número de parámetros estimados y n el tamaño muestral. Las observaciones con un valor (2 o 3 veces la media) hat alto se consideran extremas.

```{r}
hat.plot <- function(fit) {
  p <- length(coefficients(fit))
  n <- length(fitted(fit))
  plot(hatvalues(fit), main="Index Plot of Hat Values")
  abline(h=c(2,3)*p/n, col="red", lty=2)
  identify(1:n, hatvalues(fit), names(hatvalues(fit)))
}
hat.plot(regres01)
```
###### Valores Influyentes
Hay dos métodos par identificar observaciones influyentes:

- La distancia de Cook (D-estadístico)
- Gráficos added variable
```{r}
# Cooks Distance D
# identify D values > 4/(n-k-1) 
cutoff <- 4/(nrow(nba)-length(regres01$coefficients)-2)
plot(regres01, which=4, cook.levels=cutoff)
abline(h=cutoff, lty=2, col="red")
```

```{r linealidad-plot, warning = FALSE}
# Added variable plots
# add id.method="identify" to interactively identify points
avPlots(regres01, ask = FALSE, id.method = "identify")
```

```{r influence-plot, warning = FALSE}
# Influence Plot
influencePlot(regres01, id.method="identify", main="Influence Plot", 
              sub="Circle size is proportial to Cook's Distance" )
```

#### Vamos a verificar si merece la pena hacer una transformacion de las variables para ajustar el modelo

La funcion powerTransform nos indica si merece la pena hacer una transformacion polinomial en la variable explicativa para ajustar el modelo.

```{r}

summary(powerTransform(nba$age))  
#no vale la pena transformar la variable edad para que cumpla la normalidad porque el p valor nos indica que no es necesario
summary(powerTransform(nba$nba_draftnumber))
#no vale la pena transformar la variable draftnumber para que cumpla la normalidad porque el p valor nos indica que no es necesario
summary(powerTransform(nba$g)) 
#no vale la pena transformar la variable g para que cumpla la normalidad porque el p valor nos indica que no es necesario
summary(powerTransform(nba$mp))
```

#### Métodos de Selección
##### Selección Best Subset
##### Selección Stepwise
- Forward Stepwise
- Backward Stepwise
- Mixto
```{r}
library(MASS)
stepAIC(regres01, direction = "both")
```


#### Hacemos un summary al modelo con el menor AIC para ver su R2


```{r}
summary(lm(formula = log(salary, base = 10) ~ nba_draftnumber + age + 
    mp + per + ts + trb + ast + tov + usg + dws + ws_48 + obpm + 
    bpm, data = nba))
```


#### Cross-Validation

In cross-validation, a portion of the data is selected as the training sample, and a portion is selected as the hold-out sample. A regression equation is developed on the training sample and then applied to the hold-out sample. Because the hold-out sam- ple wasn’t involved in the selection of the model parameters, the performance on this sample is a more accurate estimate of the operating characteristics of the model with new data.

In k-fold cross-validation, the sample is divided into k subsamples. Each of the k sub- samples serves as a hold-out group, and the combined observations from the remaining k – 1 subsamples serve as the training group. The performance for the k prediction equations applied to the k hold-out samples is recorded and then averaged. (When k equals n, the total number of observations, this approach is called jackknifing.)

You can perform k-fold cross-validation using the crossval() function in the bootstrap package. The following listing provides a function (called shrinkage()) for cross-validating a model’s R-square statistic using k-fold cross-validation.

```{r}


shrinkage <- function(fit, k=10){ require(bootstrap)
theta.fit <- function(x,y){lsfit(x,y)}
theta.predict <- function(fit,x){cbind(1,x)%*%fit$coef}
x <- fit$model[,2:ncol(fit$model)]
y <- fit$model[,1]
results <- crossval(x, y, theta.fit, theta.predict, ngroup=k)
r2 <- cor(y, fit$fitted.values)^2
r2cv <- cor(y, results$cv.fit)^2
cat("Original R-square =", r2, "\n")
cat(k, "Fold Cross-Validated R-square =", r2cv, "\n")
cat("Change =", r2-r2cv, "\n")
}
states <- as.data.frame(nba)
fit <- lm(formula = log(salary, base = 10) ~ nba_draftnumber + age + 
    mp + per + ts + trb + ast + tov + usg + dws + ws_48 + obpm + 
    bpm, data=states)
shrinkage(fit)

```
You can see that the R-square based on the sample (0.5534) is overly optimistic. A better estimate of the amount of variance in salary that this model will account for with new data is the cross-validated R-square (0.5019). (Note that observations are assigned to the k groups randomly, so you’ll get a slightly different result each time you execute the shrinkage() function.)


#### Relative importance of each variable 

“Which variables are most important in predicting the outcome?” You implicitly want to rank-order the predictors in terms of relative importance. There may be practical grounds for asking the second question. For example, if you could rank-order leadership practices by their relative importance for organizational success, you could help managers focus on the behaviors they most need to develop.

There have been many attempts to develop a means for assessing the relative importance of predictors. The simplest has been to compare standardized regression coefficients. Standardized regression coefficients describe the expected change in the response variable (expressed in standard deviation units) for a standard deviation change in a predictor variable, holding the other predictor variables constant. You can obtain the standardized regression coefficients in R by standardizing each of the variables in your dataset to a mean of 0 and standard deviation of 1 using the scale() function, before submitting the dataset to a regression analysis.

```{r}
relweights <- function(fit,...){
R <- cor(fit$model)
nvar <- ncol(R)
rxx <- R[2:nvar, 2:nvar]
rxy <- R[2:nvar, 1]
svd <- eigen(rxx)
evec <- svd$vectors
ev <- svd$values
delta <- diag(sqrt(ev))
lambda <- evec %*% delta %*% t(evec)
lambdasq <- lambda ^ 2
beta <- solve(lambda) %*% rxy
rsquare <- colSums(beta ^ 2)
rawwgt <- lambdasq %*% beta ^ 2
import <- (rawwgt / rsquare) * 100
import <- as.data.frame(import)
row.names(import) <- names(fit$model[2:nvar]) 
names(import) <- "Weights"
import <- import[order(import),1, drop=FALSE]
dotchart(import$Weights, labels=row.names(import),
  xlab="% of R-Square", pch=19,
  main="Relative Importance of Predictor Variables", 
  sub=paste("Total R-Square=", round(rsquare, digits=3)), ...)
return(import)
}

fit <- lm(formula = log(salary, base = 10) ~ nba_draftnumber + age + 
    mp + per + ts + trb + ast + tov + usg + dws + ws_48 + obpm + 
    bpm, data = nba)
relweights(fit, col="blue")
```



### Tomamos una muestra aleatoria con la funcion set.seed

```{r}
set.seed(1234)
n <- 10
ind <- sample(1:nrow(nba), n, replace = FALSE)
muestra <- nba[ind,]
muestra <- data.frame(muestra)
muestra
```



```{r}
prediccion <- predict(fit, newdata = muestra)
prediccion
```


## Vamos a utilizar los Cross-Validation con los metodos Ridge, Lasso y Elastic Net para tomar el mejor modelo y luego haremos prediccion

### Leave-One-Out Cross-Validation

## Probamos el cv error con los modelos estimados por lm

```{r}
library(glmnet)
library (boot)
library(skimr)
log_data <- nba %>% mutate(salary=log(salary))

vars <- c("player","nba_country","tm", "country", "team")

nba_sincategoricas <- log_data %>% select_at(vars(-vars))

set.seed(123)
mod1_fit1 = glm(salary ~ .,data = nba_sincategoricas,family = gaussian())
coef(mod1_fit1)
```

```{r}
cv.err =cv.glm(nba_sincategoricas, mod1_fit1)
#delta  
# A vector of length two. The first component is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate. The adjustment is designed to compensate for the bias introduced by not using leave-one-out cross-validation.
cv.err$delta
```

```{r}
#vemos el CV error con el modelo estimado por el step AIC "both"
fit <- glm(formula = log(salary, base = 10) ~ nba_draftnumber + age + mp + per + ts + trb + ast + tov + usg + dws + ws_48 + obpm + bpm, data = nba, family = gaussian())
cv.err2 = cv.glm(nba, fit)
cv.err2$delta

#tiene menor cv error el modelo estimado con el AIC
```
## K-Fold Cross-Validation
Supone dividir la muestra en k grupos o folds, de aproximadamente igual tamaño. Cada folds es tratado como un conjunto de validación, de tal forma que se estima el modelo con los datos que no están el fold (los otros k−1 folds) y se predicen en el fold.


```{r}
#k folds cross validation con todas las variables explicativas menos las categoricas
set.seed(123)
cv.err = cv.glm(nba_sincategoricas, mod1_fit1, K = 10)
cv.err$delta
```

```{r}
#k folds cross validation con el modelo fit estimado por AIC 
cv.err2 =cv.glm(nba, fit, K = 10)
cv.err2$delta

#observamos que tiene menor error CV
```

## REGULARIZACION Y METODOS DE CONTRACCION

### Ridge

```{r}
library(rsample)  # data splitting 
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
```

```{r}
# Create training (70%) and test (30%) sets for nba data set.
# Use set.seed for reproducibility

set.seed(05112020)
nba_split <- initial_split(nba_sincategoricas, prop = .7, strata = "salary")
nba_train <- training(nba_split)
nba_test  <- testing(nba_split)
```

```{r}
# Create training and testing feature model matrices and response vectors.
# we use model.matrix(...)[, -1] to discard the intercept
nba_train_x <- model.matrix(salary ~ ., nba_train)[, -1]
nba_train_y <- nba_train$salary

nba_test_x <- model.matrix(salary ~ ., nba_test)[, -1]
nba_test_y <- nba_test$salary

# What is the dimension of of your feature matrix?
dim(nba_train_x)
```

```{r}
# Apply Ridge regression to nba data
nba_ridge <- glmnet(
  x = nba_train_x,
  y = nba_train_y,
  alpha = 0
)

plot(nba_ridge, xvar = "lambda")
```

```{r}
# lambdas applied to penalty parameter
nba_ridge$lambda %>% head()
```


### Tuning λ

```{r}
# Apply CV Ridge regression to nba data
nba_ridge_cv <- cv.glmnet(
  x = nba_train_x,
  y = nba_train_y,
  alpha = 0
)

# plot results
plot(nba_ridge_cv)
```

```{r}
min(nba_ridge_cv$cvm) # minimum MSE
```


```{r}
nba_ridge_cv$lambda.min     # lambda for this min MSE
```


```{r}
log(nba_ridge_cv$lambda.min) #calculo el logaritmo para verlo visual en el grafico
```

```{r}
nba_ridge_cv$cvm[nba_ridge_cv$lambda == nba_ridge_cv$lambda.1se]  # 1 st.error of min MSE
```

```{r}
nba_ridge_cv$lambda.1se  # lambda for this MSE
```

```{r}
log(nba_ridge_cv$lambda.1se)
```

```{r}
plot(nba_ridge, xvar = "lambda")
abline(v = log(nba_ridge_cv$lambda.1se), col = "red", lty = "dashed")
```

##### Ventajas y Desventajas

```{r}
coef(nba_ridge_cv, s = "lambda.1se") %>%
  broom::tidy() %>%
  filter(row != "(Intercept)") %>%
  top_n(25, wt = abs(value)) %>%
  ggplot(aes(value, reorder(row, value))) +
  geom_point() +
  ggtitle("Top 25 influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```


### Lasso

```{r}
## Apply lasso regression to ames data: alpha=1
nba_lasso <- glmnet(
  x = nba_train_x,
  y = nba_train_y,
  alpha = 1
)

plot(nba_lasso, xvar = "lambda")
```

### Tuning - CV

```{r}
# Apply CV Ridge regression to ames data
nba_lasso_cv <- cv.glmnet(
  x = nba_train_x,
  y = nba_train_y,
  alpha = 1
)
# plot results
plot(nba_lasso_cv)
```

```{r}
min(nba_lasso_cv$cvm)   # minimum MSE
```

```{r}
nba_lasso_cv$lambda.min     # lambda for this min MSE
```

```{r}
log(nba_lasso_cv$lambda.min)  #calculo el logaritmo para verlo en el grafico
```

```{r}
nba_lasso_cv$cvm[nba_lasso_cv$lambda == nba_lasso_cv$lambda.1se]  # 1 st.error of min MSE
```

```{r}
nba_lasso_cv$lambda.1se  # lambda for this MSE
```

```{r}
log(nba_lasso_cv$lambda.1se) #calculo el logaritmo del lambda
```


```{r}
plot(nba_lasso, xvar = "lambda")
abline(v = log(nba_lasso_cv$lambda.min), col = "red", lty = "dashed")
abline(v = log(nba_lasso_cv$lambda.1se), col = "red", lty = "dashed")
```
### Ventajas y Desventajas

```{r}
coef(nba_lasso_cv, s = "lambda.1se") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  ggplot(aes(value, reorder(row, value), color = value > 0)) +
  geom_point(show.legend = FALSE) +
  ggtitle("Influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```

```{r}
# minimum Ridge MSE
min(nba_ridge_cv$cvm)
```


```{r}
# minimum Lasso MSE
min(nba_lasso_cv$cvm)
```

## Elastic Net (Red elástica)

La red elástica es otra penalización que incorpora la selección variable del lazo y la contracción de predictores correlacionados como la regresión de ridge.

```{r}
lasso    <- glmnet(nba_train_x, nba_train_y, alpha = 1.0) 
elastic1 <- glmnet(nba_train_x, nba_train_y, alpha = 0.25) 
elastic2 <- glmnet(nba_train_x, nba_train_y, alpha = 0.75) 
ridge    <- glmnet(nba_train_x, nba_train_y, alpha = 0.0)

par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
```

### Tuning
Elastic nets: λ y α

```{r}
# maintain the same folds across all models
fold_id <- sample(1:10, size = length(nba_train_y), replace=TRUE)

# search across a range of alphas
tuning_grid <- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)
tuning_grid
```


```{r}
for(i in seq_along(tuning_grid$alpha)) {
  
  # fit CV model for each alpha value
  fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
  
  # extract MSE and lambda values
  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] <- fit$lambda.min
  tuning_grid$lambda_1se[i] <- fit$lambda.1se
}

tuning_grid
```

```{r}
tuning_grid %>%
  mutate(se = mse_1se - mse_min) %>%
  ggplot(aes(alpha, mse_min)) +
  geom_line(size = 2) +
  geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
  ggtitle("MSE ± one standard error")
```
Influential variables in the elastic net model

```{r}
coef(fit, s = "lambda.1se") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  ggplot(aes(value, reorder(row, value), color = value > 0)) +
  geom_point(show.legend = FALSE) +
  ggtitle("Influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```


Use the plotres() function to plot model diagnostics for glmnet models

```{r}
# install.packages("plotmo")
library(plotmo)
plotres(nba_ridge)
plotres(nba_lasso)
plotres(fit)
# The first plot shows the estimated slope for each parameter for # different values of (log) lambda. Notice the different shape
# between ridge and LASSO.
```


## Predicción
Vamos a comparar la prediccion de cada uno de los metodos y utilizaremos el que tiene menor mse.test.mean

```{r}
# some best model
cv_ridge   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 0.0)
min(cv_ridge$cvm)
```

```{r}
# predict
pred <- predict(cv_ridge, s = cv_ridge$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)   # prediccion con ridge
```


```{r}
# some best model
cv_lasso   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 1.0)
min(cv_lasso$cvm)
```

```{r}
# predict
pred <- predict(cv_lasso, s = cv_lasso$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)   # prediccion con lasso
```

```{r}
# some best model
cv_net   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 0.2)
min(cv_net$cvm)     
```

```{r}
# predict
pred <- predict(cv_net, s = cv_net$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)    #prediccion con elastic net
```

Vamos a elegir el modelo Elastic net ya que es el modelo mas simple junto con lasso pero menor mse.test.mean que lasso. El modelo ridge lo rechazo porq no elimina variables explicativas del modelo.

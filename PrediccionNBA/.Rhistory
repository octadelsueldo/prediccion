tuning_grid
for(i in seq_along(tuning_grid$alpha)) {
# fit CV model for each alpha value
fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
# extract MSE and lambda values
tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
tuning_grid$lambda_min[i] <- fit$lambda.min
tuning_grid$lambda_1se[i] <- fit$lambda.1se
}
tuning_grid
tuning_grid %>%
mutate(se = mse_1se - mse_min) %>%
ggplot(aes(alpha, mse_min)) +
geom_line(size = 2) +
geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
ggtitle("MSE ± one standard error")
coef(fit, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
ggplot(aes(value, reorder(row, value), color = value > 0)) +
geom_point(show.legend = FALSE) +
ggtitle("Influential variables") +
xlab("Coefficient") +
ylab(NULL)
# install.packages("plotmo")
library(plotmo)
plotres(nba_ridge)
plotres(nba_lasso)
plotres(fit)
# The first plot shows the estimated slope for each parameter for # different values of (log) lambda. Notice the different shape
# between ridge and LASSO.
# some best model
cv_ridge   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 0.0)
min(cv_ridge$cvm)
# predict
pred <- predict(cv_ridge, s = cv_ridge$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)   # prediccion con ridge
# some best model
cv_lasso   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 1.0)
min(cv_lasso$cvm)
# predict
pred <- predict(cv_lasso, s = cv_lasso$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)   # prediccion con lasso
# some best model
cv_net   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 0.2)
min(cv_net$cvm)
# predict
pred <- predict(cv_net, s = cv_net$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)    #prediccion con elastic net
knitr::opts_chunk$set(echo = TRUE)
nba %<>% clean_names()
library(readr)
library(dplyr)
library(ggplot2)
library(knitr)
library(gvlma)
library(MASS)
library(car)
library(bootstrap)
library(prettydoc)
library(here) # Comentar
library(tidyverse)
library(janitor) # Clean names
library(skimr) # Beautiful Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
nba <- read_csv("nba.csv")
nba %<>% clean_names()
colnames(nba)
skim(nba)
# delete duplicate
# Remove duplicate rows of the dataframe
nba %<>% distinct(player,.keep_all= TRUE)
# delete NA's
nba %<>% drop_na()
# Summarise
skim(nba)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(ggplot2)
library(knitr)
library(gvlma)
library(MASS)
library(car)
library(bootstrap)
library(prettydoc)
library(here) # Comentar
library(tidyverse)
library(janitor) # Clean names
library(skimr) # Beautiful Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
nba <- read_csv("nba.csv")
nba %<>% clean_names()
colnames(nba)
skim(nba)
# delete duplicate
# Remove duplicate rows of the dataframe
nba %<>% distinct(player,.keep_all= TRUE)
# delete NA's
nba %<>% drop_na()
# Summarise
skim(nba)
library(glmnet)
library (boot)
library(skimr)
log_data <- nba %>% mutate(salary=log(salary))
vars <- c("player","nba_country","tm")
nba_sincategoricas <- log_data %>% select_at(vars(-vars))
set.seed(123)
mod1_fit1 = glm(salary ~ .,data = nba_sincategoricas,family = gaussian())
coef(mod1_fit1)
cv.err =cv.glm(nba_sincategoricas, mod1_fit1)
#delta
# A vector of length two. The first component is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate. The adjustment is designed to compensate for the bias introduced by not using leave-one-out cross-validation.
cv.err$delta
#vemos el CV error con el modelo estimado por el step AIC "both"
fit <- glm(formula = log(salary, base = 10) ~ nba_draftnumber + age + mp + per + ts + trb + ast + tov + usg + dws + ws_48 + obpm + bpm, data = nba, family = gaussian())
cv.err =cv.glm(nba_sincategoricas, mod1_fit1)
#delta
# A vector of length two. The first component is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate. The adjustment is designed to compensate for the bias introduced by not using leave-one-out cross-validation.
cv.err$delta
#k folds cross validation con todas las variables explicativas menos las categoricas
set.seed(123)
cv.err = cv.glm(nba_sincategoricas, mod1_fit1, K = 10)
cv.err$delta
#k folds cross validation con el modelo fit estimado por AIC
cv.err2 =cv.glm(nba, fit, K = 10)
#k folds cross validation con todas las variables explicativas menos las categoricas
set.seed(123)
cv.err = cv.glm(nba_sincategoricas, mod1_fit1, K = 10)
cv.err$delta
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
# Create training (70%) and test (30%) sets for nba data set.
# Use set.seed for reproducibility
set.seed(05112020)
nba_split <- initial_split(nba_sincategoricas, prop = .7, strata = "salary")
nba_train <- training(nba_split)
nba_test  <- testing(nba_split)
# Create training and testing feature model matrices and response vectors.
# we use model.matrix(...)[, -1] to discard the intercept
nba_train_x <- model.matrix(salary ~ ., nba_train)[, -1]
nba_train_y <- nba_train$salary
nba_test_x <- model.matrix(salary ~ ., nba_test)[, -1]
nba_test_y <- nba_test$salary
# What is the dimension of of your feature matrix?
dim(nba_train_x)
# Apply Ridge regression to nba data
nba_ridge <- glmnet(
x = nba_train_x,
y = nba_train_y,
alpha = 0
)
plot(nba_ridge, xvar = "lambda")
# lambdas applied to penalty parameter
nba_ridge$lambda %>% head()
# Apply CV Ridge regression to nba data
nba_ridge_cv <- cv.glmnet(
x = nba_train_x,
y = nba_train_y,
alpha = 0
)
# plot results
plot(nba_ridge_cv)
min(nba_ridge_cv$cvm) # minimum MSE
nba_ridge_cv$lambda.min     # lambda for this min MSE
log(nba_ridge_cv$lambda.min) #calculo el logaritmo para verlo visual en el grafico
nba_ridge_cv$cvm[nba_ridge_cv$lambda == nba_ridge_cv$lambda.1se]  # 1 st.error of min MSE
nba_ridge_cv$lambda.1se  # lambda for this MSE
log(nba_ridge_cv$lambda.1se)
plot(nba_ridge, xvar = "lambda")
abline(v = log(nba_ridge_cv$lambda.1se), col = "red", lty = "dashed")
coef(nba_ridge_cv, s = "lambda.1se") %>%
broom::tidy() %>%
filter(row != "(Intercept)") %>%
top_n(25, wt = abs(value)) %>%
ggplot(aes(value, reorder(row, value))) +
geom_point() +
ggtitle("Top 25 influential variables") +
xlab("Coefficient") +
ylab(NULL)
## Apply lasso regression to ames data: alpha=1
nba_lasso <- glmnet(
x = nba_train_x,
y = nba_train_y,
alpha = 1
)
plot(nba_lasso, xvar = "lambda")
# Apply CV Ridge regression to ames data
nba_lasso_cv <- cv.glmnet(
x = nba_train_x,
y = nba_train_y,
alpha = 1
)
# plot results
plot(nba_lasso_cv)
min(nba_lasso_cv$cvm)   # minimum MSE
nba_lasso_cv$lambda.min     # lambda for this min MSE
log(nba_lasso_cv$lambda.min)  #calculo el logaritmo para verlo en el grafico
nba_lasso_cv$cvm[nba_lasso_cv$lambda == nba_lasso_cv$lambda.1se]  # 1 st.error of min MSE
nba_lasso_cv$lambda.1se  # lambda for this MSE
log(nba_lasso_cv$lambda.1se) #calculo el logaritmo del lambda
plot(nba_lasso, xvar = "lambda")
abline(v = log(nba_lasso_cv$lambda.min), col = "red", lty = "dashed")
abline(v = log(nba_lasso_cv$lambda.1se), col = "red", lty = "dashed")
coef(nba_lasso_cv, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
ggplot(aes(value, reorder(row, value), color = value > 0)) +
geom_point(show.legend = FALSE) +
ggtitle("Influential variables") +
xlab("Coefficient") +
ylab(NULL)
# minimum Ridge MSE
min(nba_ridge_cv$cvm)
# minimum Lasso MSE
min(nba_lasso_cv$cvm)
lasso    <- glmnet(nba_train_x, nba_train_y, alpha = 1.0)
elastic1 <- glmnet(nba_train_x, nba_train_y, alpha = 0.25)
elastic2 <- glmnet(nba_train_x, nba_train_y, alpha = 0.75)
ridge    <- glmnet(nba_train_x, nba_train_y, alpha = 0.0)
par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
# maintain the same folds across all models
fold_id <- sample(1:10, size = length(nba_train_y), replace=TRUE)
# search across a range of alphas
tuning_grid <- tibble::tibble(
alpha      = seq(0, 1, by = .1),
mse_min    = NA,
mse_1se    = NA,
lambda_min = NA,
lambda_1se = NA
)
tuning_grid
for(i in seq_along(tuning_grid$alpha)) {
# fit CV model for each alpha value
fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
# extract MSE and lambda values
tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
tuning_grid$lambda_min[i] <- fit$lambda.min
tuning_grid$lambda_1se[i] <- fit$lambda.1se
}
tuning_grid
tuning_grid %>%
mutate(se = mse_1se - mse_min) %>%
ggplot(aes(alpha, mse_min)) +
geom_line(size = 2) +
geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
ggtitle("MSE ± one standard error")
coef(fit, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
ggplot(aes(value, reorder(row, value), color = value > 0)) +
geom_point(show.legend = FALSE) +
ggtitle("Influential variables") +
xlab("Coefficient") +
ylab(NULL)
# install.packages("plotmo")
library(plotmo)
plotres(nba_ridge)
plotres(nba_lasso)
plotres(fit)
# The first plot shows the estimated slope for each parameter for # different values of (log) lambda. Notice the different shape
# between ridge and LASSO.
# install.packages("plotmo")
library(plotmo)
plotres(nba_ridge)
plotres(nba_lasso)
plotres(fit)
# The first plot shows the estimated slope for each parameter for # different values of (log) lambda. Notice the different shape
# between ridge and LASSO.
# some best model
cv_ridge   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 0.0)
min(cv_ridge$cvm)
# predict
pred <- predict(cv_ridge, s = cv_ridge$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)   # prediccion con ridge
# some best model
cv_lasso   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 1.0)
min(cv_lasso$cvm)
# predict
pred <- predict(cv_lasso, s = cv_lasso$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)   # prediccion con lasso
# some best model
cv_net   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 0.2)
min(cv_net$cvm)
# predict
pred <- predict(cv_net, s = cv_net$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)    #prediccion con elastic net
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(knitr)
library(gvlma)
library(MASS)
library(car)
library(bootstrap)
library(prettydoc)
library(here) # Comentar
library(tidyverse)
library(janitor) # Clean names
library(skimr) # Beautiful Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
library(boot)
library(plotmo)
library(rsample)  # data splitting
nba <- read_csv("nba.csv")
nba %<>% clean_names()
colnames(nba)
skim(nba)
# delete duplicate
# Remove duplicate rows of the dataframe
nba %<>% distinct(player,.keep_all= TRUE)
# delete NA's
nba %<>% drop_na()
# Summarise
skim(nba)
#library(glmnet)
#library (boot)
#library(skimr)
log_data <- nba %>% mutate(salary=log(salary))
vars <- c("player","nba_country","tm")
nba_sincategoricas <- log_data %>% select_at(vars(-vars))
set.seed(123)
mod1_fit1 = glm(salary ~ .,data = nba_sincategoricas,family = gaussian())
coef(mod1_fit1)
cv.err =cv.glm(nba_sincategoricas, mod1_fit1)
#delta
# A vector of length two. The first component is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate. The adjustment is designed to compensate for the bias introduced by not using leave-one-out cross-validation.
cv.err$delta
#k folds cross validation con todas las variables explicativas menos las categoricas
set.seed(123)
cv.err = cv.glm(nba_sincategoricas, mod1_fit1, K = 10)
cv.err$delta
#library(rsample)  # data splitting
#library(glmnet)   # implementing regularized regression approaches
#library(dplyr)    # basic data manipulation procedures
#library(ggplot2)  # plotting
# Create training (70%) and test (30%) sets for nba data set.
# Use set.seed for reproducibility
set.seed(05112020)
nba_split <- initial_split(nba_sincategoricas, prop = .7, strata = "salary")
nba_train <- training(nba_split)
nba_test  <- testing(nba_split)
# Create training and testing feature model matrices and response vectors.
# we use model.matrix(...)[, -1] to discard the intercept
nba_train_x <- model.matrix(salary ~ ., nba_train)[, -1]
nba_train_y <- nba_train$salary
nba_test_x <- model.matrix(salary ~ ., nba_test)[, -1]
nba_test_y <- nba_test$salary
# What is the dimension of of your feature matrix?
dim(nba_train_x)
# Apply Ridge regression to nba data
nba_ridge <- glmnet(
x = nba_train_x,
y = nba_train_y,
alpha = 0
)
plot(nba_ridge, xvar = "lambda")
# lambdas applied to penalty parameter
nba_ridge$lambda %>% head()
# Apply CV Ridge regression to nba data
nba_ridge_cv <- cv.glmnet(
x = nba_train_x,
y = nba_train_y,
alpha = 0
)
# plot results
plot(nba_ridge_cv)
min(nba_ridge_cv$cvm) # minimum MSE
nba_ridge_cv$lambda.min     # lambda for this min MSE
log(nba_ridge_cv$lambda.min) #calculo el logaritmo para verlo visual en el grafico
nba_ridge_cv$cvm[nba_ridge_cv$lambda == nba_ridge_cv$lambda.1se]  # 1 st.error of min MSE
nba_ridge_cv$lambda.1se  # lambda for this MSE
log(nba_ridge_cv$lambda.1se)
plot(nba_ridge, xvar = "lambda")
abline(v = log(nba_ridge_cv$lambda.1se), col = "red", lty = "dashed")
coef(nba_ridge_cv, s = "lambda.1se") %>%
broom::tidy() %>%
filter(row != "(Intercept)") %>%
top_n(25, wt = abs(value)) %>%
ggplot(aes(value, reorder(row, value))) +
geom_point() +
ggtitle("Top 25 influential variables") +
xlab("Coefficient") +
ylab(NULL)
## Apply lasso regression to ames data: alpha=1
nba_lasso <- glmnet(
x = nba_train_x,
y = nba_train_y,
alpha = 1
)
plot(nba_lasso, xvar = "lambda")
# Apply CV Ridge regression to ames data
nba_lasso_cv <- cv.glmnet(
x = nba_train_x,
y = nba_train_y,
alpha = 1
)
# plot results
plot(nba_lasso_cv)
min(nba_lasso_cv$cvm)   # minimum MSE
nba_lasso_cv$lambda.min     # lambda for this min MSE
log(nba_lasso_cv$lambda.min)  #calculo el logaritmo para verlo en el grafico
nba_lasso_cv$cvm[nba_lasso_cv$lambda == nba_lasso_cv$lambda.1se]  # 1 st.error of min MSE
nba_lasso_cv$lambda.1se  # lambda for this MSE
log(nba_lasso_cv$lambda.1se) #calculo el logaritmo del lambda
plot(nba_lasso, xvar = "lambda")
abline(v = log(nba_lasso_cv$lambda.min), col = "red", lty = "dashed")
abline(v = log(nba_lasso_cv$lambda.1se), col = "red", lty = "dashed")
coef(nba_lasso_cv, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
ggplot(aes(value, reorder(row, value), color = value > 0)) +
geom_point(show.legend = FALSE) +
ggtitle("Influential variables") +
xlab("Coefficient") +
ylab(NULL)
# minimum Ridge MSE
min(nba_ridge_cv$cvm)
# minimum Lasso MSE
min(nba_lasso_cv$cvm)
lasso    <- glmnet(nba_train_x, nba_train_y, alpha = 1.0)
elastic1 <- glmnet(nba_train_x, nba_train_y, alpha = 0.25)
elastic2 <- glmnet(nba_train_x, nba_train_y, alpha = 0.75)
ridge    <- glmnet(nba_train_x, nba_train_y, alpha = 0.0)
par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
# maintain the same folds across all models
fold_id <- sample(1:10, size = length(nba_train_y), replace=TRUE)
# search across a range of alphas
tuning_grid <- tibble::tibble(
alpha      = seq(0, 1, by = .1),
mse_min    = NA,
mse_1se    = NA,
lambda_min = NA,
lambda_1se = NA
)
tuning_grid
for(i in seq_along(tuning_grid$alpha)) {
# fit CV model for each alpha value
fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
# extract MSE and lambda values
tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
tuning_grid$lambda_min[i] <- fit$lambda.min
tuning_grid$lambda_1se[i] <- fit$lambda.1se
}
tuning_grid
tuning_grid %>%
mutate(se = mse_1se - mse_min) %>%
ggplot(aes(alpha, mse_min)) +
geom_line(size = 2) +
geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
ggtitle("MSE ± one standard error")
coef(fit, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
ggplot(aes(value, reorder(row, value), color = value > 0)) +
geom_point(show.legend = FALSE) +
ggtitle("Influential variables") +
xlab("Coefficient") +
ylab(NULL)
# install.packages("plotmo")
#library(plotmo)
plotres(nba_ridge)
plotres(nba_lasso)
plotres(fit)
# The first plot shows the estimated slope for each parameter for # different values of (log) lambda. Notice the different shape
# between ridge and LASSO.
# some best model
cv_ridge   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 0.0)
min(cv_ridge$cvm)
# predict
pred <- predict(cv_ridge, s = cv_ridge$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)   # prediccion con ridge
# some best model
cv_lasso   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 1.0)
min(cv_lasso$cvm)
# predict
pred <- predict(cv_lasso, s = cv_lasso$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)   # prediccion con lasso
# some best model
cv_net   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 0.2)
min(cv_net$cvm)
# predict
pred <- predict(cv_net, s = cv_net$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)    #prediccion con elastic net
